{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NknsaUHUGWOX",
        "outputId": "21396bc4-cdfb-4786-b752-7c0f8f2e317f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do-1"
      ],
      "metadata": {
        "id": "DAQ0lpg7GycC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#1 Read and Observe the Dataset.\n",
        "file = \"/content/drive/MyDrive/Datasets-20241203T015332Z-001/Datasets/student.csv\"\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "#2 Print top(5) and bottom(5) of the dataset\n",
        "print(\"Top 5 of dataset: \\n\", df.head(5))\n",
        "print(\"\\n\")\n",
        "print(\"Bottom 5 of dataset: \\n\", df.tail(5))\n",
        "print(\"\\n\")\n",
        "\n",
        "#3 Print the Information of Datasets.\n",
        "print(\"Information of dataset: \\n\", df.info)\n",
        "print(\"\\n\")\n",
        "\n",
        "#4 Gather the Descriptive info about the Dataset.\n",
        "print(\"Description of dataset: \\n\", df.describe)\n",
        "print(\"\\n\")\n",
        "\n",
        "#5 Split your data into Feature (X) and Label (Y).\n",
        "X = df[['Math', 'Reading']]\n",
        "Y = df['Writing']\n",
        "print(\"\\nData has been split into features and labels.\")\n",
        "print(\"Features (X):\")\n",
        "print(X.head())\n",
        "print(\"\\nLabel (Y):\")\n",
        "print(Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4--fa80fGyCp",
        "outputId": "0b9b65dc-7a37-4a93-8b19-4e499496b889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 of dataset: \n",
            "    Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "\n",
            "Bottom 5 of dataset: \n",
            "      Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "\n",
            "Information of dataset: \n",
            " <bound method DataFrame.info of      Math  Reading  Writing\n",
            "0      48       68       63\n",
            "1      62       81       72\n",
            "2      79       80       78\n",
            "3      76       83       79\n",
            "4      59       64       62\n",
            "..    ...      ...      ...\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "[1000 rows x 3 columns]>\n",
            "\n",
            "\n",
            "Description of dataset: \n",
            " <bound method NDFrame.describe of      Math  Reading  Writing\n",
            "0      48       68       63\n",
            "1      62       81       72\n",
            "2      79       80       78\n",
            "3      76       83       79\n",
            "4      59       64       62\n",
            "..    ...      ...      ...\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "[1000 rows x 3 columns]>\n",
            "\n",
            "\n",
            "\n",
            "Data has been split into features and labels.\n",
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label (Y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do-2"
      ],
      "metadata": {
        "id": "nkiWk-ixI_tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file = \"/content/drive/MyDrive/Datasets-20241203T015332Z-001/Datasets/student.csv\"\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "X = df[['Math', 'Reading']].values.T\n",
        "W = np.random.randn(X.shape[0], 1)\n",
        "Y = np.dot(W.T, X)\n",
        "\n",
        "print(\"Feature matrix X (d x n):\")\n",
        "print(X)\n",
        "\n",
        "print(\"\\nWeights W (d x 1):\")\n",
        "print(W)\n",
        "\n",
        "print(\"\\nOutput Matrix Y :\")\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "ySAs1xw4JCFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37b3c3b-60ae-4a94-f2bc-7e3358f01e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix X (d x n):\n",
            "[[48 62 79 ... 89 83 66]\n",
            " [68 81 80 ... 87 82 66]]\n",
            "\n",
            "Weights W (d x 1):\n",
            "[[ 2.09529297]\n",
            " [-0.42443215]]\n",
            "\n",
            "Output Matrix Y :\n",
            "[[ 71.71267658  95.52916024 131.57357284 124.0143975   96.45862781\n",
            "  108.92291458 111.01820755  76.01073355  94.70716371 147.0894879\n",
            "  100.62234598 113.5110649   98.66139181 136.61302306  98.10262087\n",
            "   83.99434103  95.58289576  73.91544058 110.56690764 125.31456169\n",
            "  113.67227145  91.41917758 109.82551439 109.93298542 105.97875732\n",
            "  104.28102874 101.89564242  85.50399531 138.70831603  99.29531403\n",
            "  116.08452553 106.37632171 102.69077119 129.95644753 110.41115308\n",
            "   72.58840863  82.26974469 124.4656974  113.16723603  69.32729026\n",
            "  116.03079002 130.75157631 109.8792499   86.06276624  98.95148516\n",
            "  114.01610032  84.36503766 102.69077119 146.13315258  90.11901339\n",
            "  167.08608226 115.18192573  95.52916024  72.64214414  77.25716223\n",
            "   90.25335218 106.56439601  81.44774816 132.0517405  112.29150398\n",
            "  102.37381008 132.766266   167.93494655 115.63322563 111.65758176\n",
            "  143.24273084  88.47502032 165.92025685 117.0139931  113.67227145\n",
            "  116.58956095  98.58078853  88.58249135  98.97835292  98.12948863\n",
            "   73.88857282 101.47121027  42.91047821 141.65247329 116.98712534\n",
            "  137.01058745  99.71974618  88.87258471 116.08452553  58.00196113\n",
            "  117.41155748 113.93549705 119.42624718 113.14036827 122.34353668\n",
            "   91.49978085 117.78225411  95.15846361 140.00848022  93.51447055\n",
            "   83.4355701   76.35456242 107.35952479 133.56139477 138.65458051\n",
            "   55.98727143  84.36503766 110.22307877  43.28117484  73.54474395\n",
            "  119.42624718 139.98161247  79.8037551  121.94597229  55.05780387\n",
            "   91.81674197 107.6764859  156.26578855 118.52364737  69.72485464\n",
            "  106.82762161  78.90115529 112.31837174 154.17049558 142.07690543\n",
            "  106.48379274 131.57357284 119.39937942 142.13064095  65.10983656\n",
            "   80.20131948  48.40122834  87.28232716 153.85353447 161.72967092\n",
            "  129.08071548  85.0526954  105.23736406  91.4729131  104.46910305\n",
            "  129.47827987 163.4273995  150.43120955 123.13866545 152.9240669\n",
            "   87.22859164 131.89053395 112.3452395  109.00351785  88.89945247\n",
            "  100.11731056 147.99208771 146.13315258 134.94216224  72.16397648\n",
            "  114.49426798 129.45141211 112.3452395  105.71553172 104.01780314\n",
            "   71.82014761 116.85278655 108.9766501   68.79538708 117.78225411\n",
            "  117.7285186   73.3835374   71.02501884  82.55983805 123.24613648\n",
            "  126.16342598 164.64696042 106.88135713  87.62615603 169.20824298\n",
            "   86.00903073 110.22307877 123.69743639  89.77518452  92.21430635\n",
            "  120.19450819 139.92787695 116.16512881  94.70716371  83.46243785\n",
            "  124.94386506 103.19580661 125.36829721 105.26423182  77.14969119\n",
            "  111.44263969 111.46950745  90.8872744  150.0067774  152.49963476\n",
            "  101.10051364 171.75483585 116.16512881 149.23851639  62.56324369\n",
            "  114.83809685 153.74606343 113.61853593  99.74661393 137.8863195\n",
            "  149.13104535 108.15465356  96.45862781 139.18648369 145.78932371\n",
            "   42.06161392 106.9619604   68.87599035 121.5484079   74.65683384\n",
            "  112.00141063 125.81959711 142.07690543  72.61527639 107.75708918\n",
            "  169.20824298  64.28784003 113.059765    95.21219913 146.638188\n",
            "  105.31796734 117.35782197 124.12186853 136.61302306 105.15676079\n",
            "  111.04507531  82.18914142 115.68696114  89.74831676 144.59663055\n",
            "   68.8491226  105.68866397 139.55718032  95.556028   108.10091805\n",
            "  119.79694381 128.62941558  48.79879273  99.77348169 142.05003768\n",
            "  104.36163202  71.90075089  88.87258471  92.77307729  93.93890269\n",
            "  107.6764859   90.67778432  48.85252824 105.26423182  56.75553245\n",
            "  138.20328061 111.97454287  71.79327985 108.49848243 111.84020408\n",
            "  112.71593613  47.55236405  95.556028   121.5484079  111.55011072\n",
            "  104.41536753 106.93509264 111.20628185  31.74635563 106.93509264\n",
            "  165.49582471  89.66771348  93.48760279 126.58785813  80.12071621\n",
            "  168.78381084  73.91544058 111.94767511  52.56494652 137.85945174\n",
            "  104.44223529  98.58078853 142.05003768 139.18648369 107.33265703\n",
            "  118.6311184   49.56705374  74.73743711 123.69743639 114.83809685\n",
            "  130.69784079 119.45311493 109.13785664  57.09936132  83.88687\n",
            "  139.61091584 120.27511147 114.46740022 123.67056863  96.51236332\n",
            "   94.41707035  75.16186926  82.74791235 130.32714416 104.86666743\n",
            "  146.58445248 147.85774892 109.45481776  83.14547674 107.33265703\n",
            "  124.99760058 118.23355401  81.47461592 123.19240097 137.83258398\n",
            "   82.66730908 122.28980116  80.9695805  129.02697997 120.27511147\n",
            "  119.02868279 111.07194306 121.94597229  92.24117411 119.08241831\n",
            "   86.03589848 119.50685045 119.05555055 136.24232643 121.60214342\n",
            "  120.69954361 151.67763822  82.16227366 156.31952406 150.85564169\n",
            "  115.20879348 114.86496461 100.2247816   72.61527639 164.5932249\n",
            "   70.04181576 118.6311184  120.78014689 134.86155897  83.09174123\n",
            "  144.14533064  91.87047748 166.29095348  80.91584498  72.58840863\n",
            "  147.51392005 153.69232792  99.77348169 111.12567858 132.84686927\n",
            "   83.38183458  72.74961518 124.35822637 119.9312826  148.28218106\n",
            "   54.713975   132.02487274 125.26082618  96.06106342 104.7591964\n",
            "  114.78436134 162.15410306  98.60765629  97.83939527 111.57697848\n",
            "  124.97073282  76.01073355 127.83428681 111.94767511 119.47998269\n",
            "  147.46018453 122.8485721  109.00351785 132.84686927 123.24613648\n",
            "   76.35456242  93.93890269 120.30197922 128.20498343 163.40053174\n",
            "  146.21375585 165.52269247 103.96406763 113.96236481 137.48875511\n",
            "  123.27300424 103.11520334  72.19084424 102.29320681 138.25701613\n",
            "  101.92251018  93.46073503  79.37932295 136.58615531 100.27851711\n",
            "   78.47672314 135.79102653  95.556028   113.5110649  105.66179621\n",
            "  150.32373851  96.03419566 112.74280389  85.98216297 127.03915803\n",
            "  129.05384772 112.74280389  99.48338834  86.37972735 105.68866397\n",
            "  147.43331677 170.48153942  97.70505648 125.28769393 126.64159364\n",
            "  110.75498195  66.41000076 131.57357284 147.83088116 117.78225411\n",
            "   70.4662479   72.98597302 130.75157631  95.2928024  132.5030404\n",
            "  149.07730984  59.22152205  55.93353592  90.06527787 149.13104535\n",
            "  141.2549089   75.64003692 155.92195968 148.76034872  70.09555127\n",
            "  106.45692498 132.79313376 124.35822637 119.05555055  73.54474395\n",
            "  128.17811568 134.49086234 112.02827839 150.35060627  98.50018526\n",
            "  129.47827987 154.19736334  71.7664121  141.28177666 111.15254634\n",
            "   99.45652058 124.94386506 122.39727219  96.06106342  76.85959784\n",
            "  125.26082618  68.47842597 134.46399458 102.26633905 154.70239876\n",
            "   98.97835292  46.64976424 105.3448351   97.38809537 125.79272935\n",
            "   97.59758545 103.96406763 137.46188735  98.12948863 127.75368353\n",
            "  131.22974397 142.07690543 101.390607   133.77633684 131.6004406\n",
            "  112.68906837 125.79272935 118.31415729 107.78395693  76.80586232\n",
            "  137.4350196   96.48549556 124.91699731 119.05555055  92.1874386\n",
            "  136.61302306  78.10602652 125.28769393  93.59507382 137.48875511\n",
            "  131.14914069  92.21430635 105.29109958 138.17641285 150.82877393\n",
            "   76.67152353 117.80912187 110.99133979  95.69036679  89.80205227\n",
            "  104.38849977  76.48890121 102.77137447  80.94271274  92.71934177\n",
            "  118.65798616 114.44053247 117.27721869 119.42624718 117.35782197\n",
            "  142.95263748 100.27851711  80.57201611  58.39952552 137.03745521\n",
            "   63.89027564 117.0139931   63.86340789 132.39556937 101.1273814\n",
            "  128.62941558 102.71763895 124.14873629 133.69573356  65.50740095\n",
            "  115.34313227 126.56099037 144.59663055 125.34142945 100.70294926\n",
            "   99.48338834  88.92632023  78.73994874  55.1652749  156.60961742\n",
            "  100.19791384 113.08663276  83.09174123  76.43516569 141.20117339\n",
            "  117.43842524  71.84701537 127.06602579 129.02697997 137.03745521\n",
            "   98.81714637 162.60540297 129.79524098 128.20498343 113.96236481\n",
            "  109.40108224  82.24287693 147.00888463  88.05058818 111.09881082\n",
            "  103.593371   118.1798185  140.00848022 113.64540369  83.56990889\n",
            "  103.96406763 136.61302306 169.23511074  89.88265555 130.32714416\n",
            "  123.58996535 109.90611766 153.34849905 113.64540369 116.79905103\n",
            "   67.17826177 103.99093539 144.22593392 119.87754708 112.31837174\n",
            "   84.28443439  84.28443439  99.85408497 109.00351785 132.90060479\n",
            "  108.89604682 101.92251018 128.71001885  84.76260205 125.26082618\n",
            "  123.24613648  85.10643092 124.89012955  76.46203345 111.52324297\n",
            "   93.11690616 167.08608226 118.73858943 135.84476205  90.1727489\n",
            "   91.84360973 139.10588042 148.28218106 167.93494655 129.55888314\n",
            "   94.30959932 116.87965431 139.58404808 150.77503842 101.07364589\n",
            "   93.06317064 107.33265703  92.26804187  91.41917758 129.95644753\n",
            "  127.80741905 150.77503842 111.07194306 177.69688588  97.78565976\n",
            "  133.72260132 132.82000151  72.72274742 107.30578927  95.58289576\n",
            "  110.24994653  70.22989006 100.14417832  94.8415025   97.67818873\n",
            "  125.71212608 146.55758472 133.66886581 115.63322563 105.55432518\n",
            "   60.46795073 130.3002764  125.26082618 132.73939824 165.07139256\n",
            "  168.35937869 121.57527566 134.96903    131.54670508 113.64540369\n",
            "   88.5556236  111.97454287 149.50174198  80.94271274 170.13771055\n",
            "  102.29320681 116.56269319 102.23947129 106.48379274 141.62560553\n",
            "  126.90481924 123.24613648 157.96351713 101.10051364 134.41025906\n",
            "  139.55718032  65.87809758 155.47065977 118.68485392 116.48208992\n",
            "  119.53371821  91.84360973 152.41903148 114.86496461 125.31456169\n",
            "   73.49100843  65.39992992 149.52860974 114.0967036   92.26804187\n",
            "  115.39686779 139.84727368  65.16357208 146.21375585 127.80741905\n",
            "   92.69247402  98.15635639 124.49256516  96.0073279  124.89012955\n",
            "  164.67382818 119.39937942  88.5556236   86.3528596  148.22844555\n",
            "  125.7658616   75.98386579 161.27837101  82.7210446  103.91033211\n",
            "  167.08608226  50.46965355 169.65954289  56.25049703  99.93468824\n",
            "   66.51747179  93.93890269  83.91373776  96.03419566 106.00562508\n",
            "   94.01950597  62.16567931  48.79879273 151.62390271  47.55236405\n",
            "   86.83102726 107.64961814  89.35075237 119.85067932 163.74436061\n",
            "   98.55392077  36.22703492 161.27837101 123.58996535  85.16016643\n",
            "  109.40108224 134.5177301  129.53201539 118.12608298 112.37210726\n",
            "  137.03745521 162.55166745  95.63663127 132.39556937 137.40815184\n",
            "   83.06487347 104.25416098  96.03419566  97.22688882 116.11139329\n",
            "  102.66390343 104.46910305  69.7517224  105.95188956 122.74110106\n",
            "  135.71042326  97.22688882  72.29831527 101.41747476 104.89353519\n",
            "  118.26042177 126.95855476 131.97113722 152.41903148 105.26423182\n",
            "  168.35937869 150.0067774  130.80531182 127.09289355 144.59663055\n",
            "  140.72300572  93.06317064 132.90060479  83.46243785 147.91148443\n",
            "  119.87754708 108.9766501  106.90822489 141.20117339 135.8716298\n",
            "  135.44719766 123.5630976   70.51998342  95.13159585  94.28273156\n",
            "   96.80245668 113.61853593  97.65132097  80.59888387  19.17459782\n",
            "   89.35075237 138.62771276 132.02487274 115.28939676 118.68485392\n",
            "   66.70009411 108.9766501  116.58956095 136.61302306 122.39727219\n",
            "   91.52664861 129.42454435  91.44604534 140.40604461 108.07405029\n",
            "  133.77633684  71.82014761  38.80049555 140.40604461  91.04848095\n",
            "  108.60595347 134.96903    162.55166745 101.04677813 133.24443366\n",
            "  160.05881009  89.721449    91.49978085 167.08608226 113.64540369\n",
            "   71.73954434 118.68485392 146.29435913  80.14758397 131.14914069\n",
            "   82.69417684  75.95699803  92.29490963 123.69743639  78.10602652\n",
            "  112.3452395  128.60254782 122.37040443  52.48434324 128.33932222\n",
            "  156.69022069 123.19240097 102.34694232  87.62615603  85.18703419\n",
            "  163.37366398  91.78987421 114.91870013 144.8867239   74.62996608\n",
            "   78.42298763 128.6831511  134.09329795 116.58956095  71.34197995\n",
            "  167.08608226  66.30252973  76.38143018 111.09881082 149.15791311\n",
            "  130.43461519 135.31285887 127.3829869   89.74831676 164.64696042\n",
            "  105.15676079  93.88516718 103.1420711  129.92957977  83.54304113\n",
            "   72.6690119  103.11520334  82.69417684 138.70831603 153.74606343\n",
            "   47.97679619 115.34313227 153.77293119 147.00888463  93.43386727\n",
            "  106.93509264 115.28939676 129.47827987  98.60765629 117.0139931\n",
            "  101.8419069  144.14533064 115.31626451 103.1420711  113.19410379\n",
            "   51.71608223 131.62730835 100.2247816   86.43346287 135.79102653\n",
            "  104.86666743 133.69573356 139.53031256 104.78606416  96.06106342\n",
            "  121.97284005 154.51432445 140.72300572 121.46780463  90.54344553\n",
            "  101.07364589 103.27640989 111.60384624  77.57412334 135.31285887\n",
            "   89.74831676 107.75708918 127.83428681  81.05018377 139.90100919\n",
            "  117.86285739 143.29646635  99.77348169 127.86115456  59.67282195\n",
            "  104.41536753 114.04296808  99.77348169 118.15295074 112.42584277\n",
            "  129.92957977 100.17104608 137.83258398  93.59507382 141.12057011\n",
            "  157.45848171  82.32348021  54.63337173 136.53241979 157.99038489\n",
            "  135.81789429  82.24287693 122.31666892 138.70831603 121.17771127\n",
            "   92.82681281  98.97835292 107.78395693 113.61853593  55.03093611\n",
            "  112.7965394   79.69628406 127.35611914 123.61683311  90.06527787\n",
            "  108.12778581  81.81844479 127.35611914 114.15043911  95.18533137\n",
            "  138.20328061 120.67267585  68.10772934 157.06091732 101.57868131\n",
            "  110.64751092 139.53031256 142.92576972 117.43842524 119.39937942\n",
            "  143.29646635 106.0593606  111.92080735 129.134451    78.82055202\n",
            "  117.27721869 100.25164935  88.18492697 110.75498195 122.31666892\n",
            "   76.03760131  91.04848095  66.27566197 140.72300572 141.59873777\n",
            "   95.21219913 123.69743639  95.18533137 101.94937793 113.56480042\n",
            "  132.84686927 126.10969047 131.62730835 110.27681429 112.76967164\n",
            "  107.75708918 118.23355401 116.11139329  87.22859164 130.24654089\n",
            "  100.19791384 139.15961593 115.66009339  63.38524023  99.34904955\n",
            "   91.84360973 133.24443366 129.05384772 120.32884698 144.96732718\n",
            "  123.16553321  71.36884771 118.20668626 108.63282122 118.68485392\n",
            "  144.1721984  108.18152132  88.07745593 106.93509264 136.95685193\n",
            "   83.91373776 119.16302158 119.74320829 106.93509264  99.45652058\n",
            "  119.45311493 116.45522216 149.5554775  139.10588042 110.27681429]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do-3"
      ],
      "metadata": {
        "id": "NyOAR_ceSSLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "file = \"/content/drive/MyDrive/Datasets-20241203T015332Z-001/Datasets/student.csv\"\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(df) * train_ratio)\n",
        "\n",
        "train_data = df.iloc[:train_size]\n",
        "test_data = df.iloc[train_size:]\n",
        "\n",
        "X_train = train_data.drop('Math', axis=1)\n",
        "y_train = train_data['Math']\n",
        "\n",
        "X_test = test_data.drop('Math', axis=1)\n",
        "y_test = test_data['Math']\n",
        "\n",
        "print(f\"Training data size (X_train): {len(X_train)} rows\")\n",
        "print(f\"Test data size (X_test): {len(X_test)} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEqbO2kFSUEy",
        "outputId": "6cb7da5b-b94c-41f0-9140-61ec88b985e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size (X_train): 800 rows\n",
            "Test data size (X_test): 200 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do"
      ],
      "metadata": {
        "id": "o9MQbfENTvul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def gradient_descent(X, y, W, learning_rate, num_iterations):\n",
        "    m = len(y)\n",
        "    for i in range(num_iterations):\n",
        "        predictions = X.dot(W)\n",
        "        errors = predictions - y\n",
        "        gradient = (1/m) * X.T.dot(errors)\n",
        "        W -= learning_rate * gradient\n",
        "        if (i+1) % 100 == 0:\n",
        "            loss = np.mean(errors**2)\n",
        "            print(f\"Iteration {i+1}: Loss = {loss}\")\n",
        "    return W\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "def r2_score_custom(y_true, y_pred):\n",
        "    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    unexplained_variance = np.sum((y_true - y_pred) ** 2)\n",
        "    return 1 - (unexplained_variance / total_variance)\n",
        "\n",
        "def train_and_evaluate_model(data, target_column, learning_rate=0.01, num_iterations=1000, train_ratio=0.8):\n",
        "    X = data.drop(target_column, axis=1).values\n",
        "    y = data[target_column].values.reshape(-1, 1)\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "    train_size = int(len(data) * train_ratio)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "    W = np.zeros((X_train.shape[1], 1))\n",
        "    print(\"Starting Gradient Descent...\")\n",
        "    W = gradient_descent(X_train, y_train, W, learning_rate, num_iterations)\n",
        "    print(\"Gradient Descent Completed.\\n\")\n",
        "    y_pred = X_test.dot(W)\n",
        "    rmse_value = rmse(y_test, y_pred)\n",
        "    r2_value = r2_score_custom(y_test, y_pred)\n",
        "    print(f\"Model Evaluation Results:\")\n",
        "    print(f\"RMSE (Root Mean Squared Error): {rmse_value:.4f}\")\n",
        "    print(f\"R² (Coefficient of Determination): {r2_value:.4f}\\n\")\n",
        "    return W\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets-20241203T015332Z-001/Datasets/student.csv\")\n",
        "W = train_and_evaluate_model(df, target_column='Math', learning_rate=0.01, num_iterations=1000)\n"
      ],
      "metadata": {
        "id": "R94yALYkTxWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e8afb4-eb3d-43aa-8694-3c220efa52b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 699.2962963635672\n",
            "Iteration 200: Loss = 162.49871532444774\n",
            "Iteration 300: Loss = 90.90732975530241\n",
            "Iteration 400: Loss = 81.3166534173156\n",
            "Iteration 500: Loss = 80.02741601616349\n",
            "Iteration 600: Loss = 79.85066715182437\n",
            "Iteration 700: Loss = 79.82332612061171\n",
            "Iteration 800: Loss = 79.81633321119472\n",
            "Iteration 900: Loss = 79.81236775342418\n",
            "Iteration 1000: Loss = 79.80908195225282\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "RMSE (Root Mean Squared Error): 8.0630\n",
            "R² (Coefficient of Determination): 0.6688\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With an R² of 0.67, the model demonstrates moderate predictive power, indicating a reasonable fit to the data, though it is not flawless.\n",
        "\n",
        "The RMSE of 8.06 indicates a moderate level of error between predictions and actual values. However, without understanding the scale of the target variable, it is challenging to determine if this error is considered large or acceptable."
      ],
      "metadata": {
        "id": "9U5zKzNpWxIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different value of learning rate\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Evaluating model with learning rate: {lr}\")\n",
        "    W = train_and_evaluate_model(df, target_column='Math', learning_rate=lr, num_iterations=1000)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "IVoPDZagV305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cf91fb-352f-43e6-e9e7-a95eb1174481"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with learning rate: 0.001\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 3889.0670099762856\n",
            "Iteration 200: Loss = 3181.745091095462\n",
            "Iteration 300: Loss = 2608.1846780395454\n",
            "Iteration 400: Loss = 2142.3042431056147\n",
            "Iteration 500: Loss = 1763.3582670188446\n",
            "Iteration 600: Loss = 1454.7681111661116\n",
            "Iteration 700: Loss = 1203.2318619387745\n",
            "Iteration 800: Loss = 998.0403036668321\n",
            "Iteration 900: Loss = 830.546782573105\n",
            "Iteration 1000: Loss = 693.7531351203352\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "RMSE (Root Mean Squared Error): 26.4877\n",
            "R² (Coefficient of Determination): -2.5742\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 0.01\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 699.2962963635672\n",
            "Iteration 200: Loss = 162.49871532444774\n",
            "Iteration 300: Loss = 90.90732975530241\n",
            "Iteration 400: Loss = 81.3166534173156\n",
            "Iteration 500: Loss = 80.02741601616349\n",
            "Iteration 600: Loss = 79.85066715182437\n",
            "Iteration 700: Loss = 79.82332612061171\n",
            "Iteration 800: Loss = 79.81633321119472\n",
            "Iteration 900: Loss = 79.81236775342418\n",
            "Iteration 1000: Loss = 79.80908195225282\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "RMSE (Root Mean Squared Error): 8.0630\n",
            "R² (Coefficient of Determination): 0.6688\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 0.1\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 79.80928990194519\n",
            "Iteration 200: Loss = 79.78923742259309\n",
            "Iteration 300: Loss = 79.7814878345864\n",
            "Iteration 400: Loss = 79.77849230088077\n",
            "Iteration 500: Loss = 79.7773344042063\n",
            "Iteration 600: Loss = 79.7768868296368\n",
            "Iteration 700: Loss = 79.7767138237031\n",
            "Iteration 800: Loss = 79.77664694981183\n",
            "Iteration 900: Loss = 79.77662110030924\n",
            "Iteration 1000: Loss = 79.77661110841598\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "RMSE (Root Mean Squared Error): 8.0551\n",
            "R² (Coefficient of Determination): 0.6694\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 0.5\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 79.77732906156535\n",
            "Iteration 200: Loss = 79.77661078235637\n",
            "Iteration 300: Loss = 79.77660486172364\n",
            "Iteration 400: Loss = 79.77660481292104\n",
            "Iteration 500: Loss = 79.77660481251877\n",
            "Iteration 600: Loss = 79.77660481251546\n",
            "Iteration 700: Loss = 79.77660481251543\n",
            "Iteration 800: Loss = 79.77660481251542\n",
            "Iteration 900: Loss = 79.77660481251543\n",
            "Iteration 1000: Loss = 79.77660481251542\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "RMSE (Root Mean Squared Error): 8.0551\n",
            "R² (Coefficient of Determination): 0.6695\n",
            "\n",
            "--------------------------------------------------\n",
            "Evaluating model with learning rate: 1\n",
            "Starting Gradient Descent...\n",
            "Iteration 100: Loss = 5724.303316873511\n",
            "Iteration 200: Loss = 202613.34169789232\n",
            "Iteration 300: Loss = 7267269.236823377\n",
            "Iteration 400: Loss = 260757069.00177982\n",
            "Iteration 500: Loss = 9356327969.944635\n",
            "Iteration 600: Loss = 335718217445.5252\n",
            "Iteration 700: Loss = 12046042292983.225\n",
            "Iteration 800: Loss = 432228956953977.25\n",
            "Iteration 900: Loss = 1.55089835056876e+16\n",
            "Iteration 1000: Loss = 5.564841631037286e+17\n",
            "Gradient Descent Completed.\n",
            "\n",
            "Model Evaluation Results:\n",
            "RMSE (Root Mean Squared Error): 694744374.0630\n",
            "R² (Coefficient of Determination): -2458907675230898.0000\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower Learning Rate (e.g., 0.001): Training will take longer, but the process may be more stable, avoiding overshooting. If the model underperforms, it could be due to insufficient learning from slow updates.\n",
        "\n",
        "Moderate Learning Rate (e.g., 0.01): Strikes a balance between training speed and convergence. This is typically a solid starting point for many models.\n",
        "\n",
        "Higher Learning Rate (e.g., 0.1, 0.5, 1): Training may be faster, but there's a greater risk of overshooting the optimal weights. If the learning rate is too high, the model might not converge properly, or it could result in significant errors (high RMSE).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hm8jyfg3XGrQ"
      }
    }
  ]
}